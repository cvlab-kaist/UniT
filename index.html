<html>

<head>
    <meta charset="utf-8" />
    <title>UniT: Unified Diffusion Transformer for High-Fidelity Text-Aware Image Restoration</title>
    <meta
        content="UniT: Unified Diffusion Transformer for High-Fidelity Text-Aware Image Restoration"
        name="description" />
    <meta
        content="UniT: Unified Diffusion Transformer for High-Fidelity Text-Aware Image Restoration"
        property="og:title" />
    <meta
        content="UniT: Unified Diffusion Transformer for High-Fidelity Text-Aware Image Restoration"
        property="og:description" />
    <meta
        content="UniT: Unified Diffusion Transformer for High-Fidelity Text-Aware Image Restoration"
        property="twitter:title" />
    <meta
        content="UniT: Unified Diffusion Transformer for High-Fidelity Text-Aware Image Restoration"
        property="twitter:description" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"
        crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&family=Varela+Round&display=swap"
        rel="stylesheet">
    <link href="style.css?" rel="stylesheet" type="text/css" />

    <!-- üîé Added minimal CSS for click‚Äëto‚Äëzoom lightbox -->
    <style>
      /* make images clearly zoomable */
      img.zoomable { cursor: zoom-in; transition: transform .2s ease; }

      /* fullscreen overlay */
      .lightbox-overlay {
        position: fixed; inset: 0; display: none; align-items: center; justify-content: center;
        background: rgba(0,0,0,.9); z-index: 10000;
      }
      .lightbox-overlay.active { display: flex; }
      .lightbox-overlay img { max-width: 95vw; max-height: 95vh; box-shadow: 0 10px 40px rgba(0,0,0,.6); border-radius: 8px; }
      /* show zoom-out cursor while overlay is open */
      .lightbox-overlay, .lightbox-overlay * { cursor: zoom-out; }

      /* prevent background scroll when overlay is open */
      body.no-scroll { overflow: hidden; }

      .title-video {
          width: 100px;  
          height: 100px; 
          object-fit: cover; 
          object-position: center;
          border-radius: 50%;
          border: 2px solid #ccc;
      }
    </style>

    <!-- MathJax for LaTeX rendering -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']]
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
</head>

<body>
    <header class="site-header">
        <div class="container">
            <nav class="main-nav">
                <ul class="nav-links">
                    <li><a href="#abstract">Overview</a></li>
                    <!-- <li><a href="#method">Method</a></li> -->
                    <li><a href="#qual-results">Qualitative Results</a></li>
                    <li><a href="#quant-results">Quantitative Results</a></li>
                    <!-- <li><a href="#ablation-studies">Ablation Studies</a></li> -->
                    <!-- <li><a href="#analysis">Analysis</a></li> -->
                    <li><a href="#citation">Citation</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <div class="hero-section">
        <div class="container">
            <div class="title-row">
                <h1 class="title" style="margin-bottom: 0.3rem;">
                    <span class="gradient-text">UniT</span>: Unified Diffusion Transformer for High-Fidelity<br>
                    Text-Aware Image Restoration
                </h1>
                <h2 class="subtitle" style="margin-top: 0;">
                    arXiv 2025
                </h2>
            </div>

            <div class="author-list">
                <div class="author-row">
                    <div class="author-col">
                        <a href="https://github.com/jinlovespho" target="_blank" class="author-text">Jin Hyeon Kim<sup>1</sup></a>
                    </div>
                    <div class="author-col">
                        <a href="#" target="_blank" class="author-text">Paul Hyunbin Cho<sup>1</sup></a>
                    </div>
                    <div class="author-col">
                        <a href="#" target="_blank" class="author-text">Claire Kim<sup>1</sup></a>
                    </div>
                </div>
                <div class="author-row">
                    <div class="author-col">
                        <a href="https://github.com/Min-Jaewon" target="_blank" class="author-text">Jaewon Min<sup>1</sup></a>
                    </div>
                    <div class="author-col">
                        <a href="https://github.com/babywhale03" target="_blank" class="author-text">Jaeeun Lee<sup>1</sup></a>
                    </div>
                    <div class="author-col">
                        <a href="#" target="_blank" class="author-text">Jihye Park<sup>2</sup></a>
                    </div>
                    <div class="author-col">
                        <a href="#" target="_blank" class="author-text">Yeji Choi<sup>1</sup></a>
                    </div>
                    <div class="author-col">
                        <a href="https://cvlab.kaist.ac.kr/" target="_blank" class="author-text">Seungryong Kim<sup>1&dagger;</sup></a>
                    </div>
                </div>
            </div>


            <div class="author-affiliations">
                <p><sup>1</sup> KAIST AI</p>
                <p><sup>2</sup> Samsung Electronics</p>
                <p><sup>&dagger;</sup> Corresponding author</p>
            </div>

            <div class="link-labels base-row">
                <div class="base-col icon-col"><a href="" target="_blank" class="link-block">
                        <i class="fa fas fa-file-text main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Paper</strong>
                    </a></div>
                <div class="base-col icon-col"><a href="https://github.com/cvlab-kaist/UniT" class="link-block">
                        <i class="fa fa-github main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Code</strong>
                    </a></div>
                <div class="base-col icon-col"><a href="#citation" class="link-block">
                        <i class="fa fa-graduation-cap main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Citation</strong>
                    </a></div>
            </div>

        </div>
    </div>

    <main class="main-content">
        <div class="container">
            <div id="abstract" class="base-row section">
                <h2>Overview of UniT</h2>
                <p class="paragraph">
                    We present <Strong>UniT</Strong> (<Strong>U</Strong>nified Diffusion <Strong>T</Strong>ransformer), a unified text-aware image restoration framework that combines a Diffusion Transformer (DiT), a Vision‚ÄìLanguage Model (VLM), and a Text Spotting Module (TSM) to perform high-fidelity text-aware image restoration. In UniT, each component serves a distinct role. The VLM extracts textual content from degraded images to provide initial textual guidance. The TSM generates intermediate OCR predictions at each denoising timestep, allowing the VLM to iteratively correct potential textual errors. The DiT-based restoration module, with its strong representational capacity, fully leverages the textual guidance to achieve fine-grained text restoration while suppressing text hallucinations.
                </p>
            </div>
            <!-- <div class="image-container">
                <div class="image-content">
                    <img class="large-image" src="images/unit_teaser.png" alt="unit_teaser_img">
                </div>
            </div> -->
            <div class="image-container">
                <div class="image-content">
                    <img src="images/fig_unit_teaser copy 15.png" alt="unit_teaser_img"
                        style="width: 100%; max-width: none;">
                </div>
            </div>


            <!-- <section id="method" class="section">
                <h2>Method</h2>
                <div class="image-container">
                    <div class="image-content">
                        <img class="medium-image" src="images/fig_unit_teaser copy 15.png" alt="unit_teaser_img">
                    </div>
                </div>
                <p>
                    Our approach integrates a strong 2D tracking backbone with additional modules designed to leverage multi-view information. 
                    Specifically, we introduce a camera encoding module to inject geometric information and a cross view-attention module to aggregate complementary cues across viewpoints. This combination allows our model to achieve robust spatio-temporal consistency across multiple views.
                </p>
            </section> -->


            <section id="qual-results" class="section">
                <h2>Qualitative Results</h2>


                <h3>Text Restoration Results on SA-Text and Real-Text Benchmarks</h3>
                <p>
                    <!-- Our method consistently boosts MLLM performance on vision-centric tasks by aligning intermediate visual features with a strong vision encoder. Gains persist even with SigLIPv2, showing that regularizing visual representations benefits MLLMs beyond compensating for contrastive pretraining limits. -->
                    We present qualitative text restoration results on the SA-Text and Real-Text benchmarks. Despite severe degradations affecting readability and style, UniT leverages a rich visual-linguistic prior, guided by precise character-level OCR predictions, to provide accurate textual guidance to the DiT restoration module, effectively recovering the degraded text. In contrast, existing methods frequently fail and often produce hallucinated text.
                </p>


                <h3>SA-Text (Level 1)</h3>
                <div class="slideshow-container">
                    <div class="slideshow">
                        <div class="slide active">
                            <img src="images/fig_qual_satext_1_1.png" alt="Qualitative Result on SA-Text (Level 1)">
                        </div>
                        <div class="slide">
                            <img src="images/fig_qual_satext_1_2.png" alt="Qualitative Result on SA-Text (Level 1)">
                        </div>
                        <div class="slide">
                            <img src="images/fig_qual_satext_1_3.png" alt="Qualitative Result on SA-Text (Level 1)">
                        </div>
                        <div class="slide">
                            <img src="images/fig_qual_satext_1_4.png" alt="Qualitative Result on SA-Text (Level 1)">
                        </div>
                    </div>

                    <button class="slideshow-nav prev" onclick="changeSlide(this, -1)">‚ùÆ</button>
                    <button class="slideshow-nav next" onclick="changeSlide(this, 1)">‚ùØ</button>
                </div>


                <h3>SA-Text (Level 2)</h3>
                <div class="slideshow-container">
                    <div class="slideshow">
                        <div class="slide active">
                            <img src="images/fig_qual_satext_2_1.png" alt="Qualitative Result on SA-Text (Level 2)">
                        </div>
                        <div class="slide">
                            <img src="images/fig_qual_satext_2_2.png" alt="Qualitative Result on SA-Text (Level 2)">
                        </div>
                        <div class="slide">
                            <img src="images/fig_qual_satext_2_3.png" alt="Qualitative Result on SA-Text (Level 2)">
                        </div>
                        <div class="slide">
                            <img src="images/fig_qual_satext_2_4.png" alt="Qualitative Result on SA-Text (Level 2)">
                        </div>
                    </div>

                    <button class="slideshow-nav prev" onclick="changeSlide(this, -1)">‚ùÆ</button>
                    <button class="slideshow-nav next" onclick="changeSlide(this, 1)">‚ùØ</button>
                </div>


                <h3>SA-Text (Level 3)</h3>
                <div class="slideshow-container">
                    <div class="slideshow">
                        <div class="slide active">
                            <img src="images/fig_qual_satext_3_1.png" alt="Qualitative Result on SA-Text (Level 3)">
                        </div>
                        <div class="slide">
                            <img src="images/fig_qual_satext_3_2.png" alt="Qualitative Result on SA-Text (Level 3)">
                        </div>
                        <div class="slide">
                            <img src="images/fig_qual_satext_3_3.png" alt="Qualitative Result on SA-Text (Level 3)">
                        </div>
                        <div class="slide">
                            <img src="images/fig_qual_satext_3_4.png" alt="Qualitative Result on SA-Text (Level 3)">
                        </div>
                    </div>

                    <button class="slideshow-nav prev" onclick="changeSlide(this, -1)">‚ùÆ</button>
                    <button class="slideshow-nav next" onclick="changeSlide(this, 1)">‚ùØ</button>
                </div>


                <h3>Real-Text</h3>
                <div class="slideshow-container">
                    <div class="slideshow">
                        <div class="slide active">
                            <img src="images/fig_qual_realtext_1.png" alt="Qualitative Result on Real-Text">
                        </div>
                        <div class="slide">
                            <img src="images/fig_qual_realtext_2.png" alt="Qualitative Result on Real-Text">
                        </div>
                        <div class="slide">
                            <img src="images/fig_qual_realtext_3.png" alt="Qualitative Result on Real-Text">
                        </div>
                        <div class="slide">
                            <img src="images/fig_qual_realtext_4.png" alt="Qualitative Result on Real-Text">
                        </div>
                    </div>
                    
                    <button class="slideshow-nav prev" onclick="changeSlide(this, -1)">‚ùÆ</button>
                    <button class="slideshow-nav next" onclick="changeSlide(this, 1)">‚ùØ</button>
                </div>

            </section>

            
            
            <section id="quant-results" class="section">
                <h2>Quantitative Results</h2>
                
                <h3>Text Restoration Results on SA-Text and Real-Text Benchmarks</h3>
                <p>
                    <!-- Our method consistently boosts MLLM performance on vision-centric tasks by aligning intermediate visual features with a strong vision encoder. Gains persist even with SigLIPv2, showing that regularizing visual representations benefits MLLMs beyond compensating for contrastive pretraining limits. -->
                    Text restoration performance is evaluated using detection and recognition (E2E) text-spotting metrics. Detection metrics measure the accuracy of locating text regions, while end-to-end (E2E) metrics assess the correctness of the recognized text compared with ground-truth annotations. Our proposed UniT framework outperforms all prior models by leveraging VLM-derived visual‚Äìlinguistic priors and TSM OCR predictions, achieving the highest restoration accuracy across all SA-Text degradation levels and the Real-Text benchmark.
                </p>
                
                <h3>SA-Text (Level 1, Level 2, Level 3)</h3>
                <div class="image-container">
                    <div class="image-content">
                        <img src="images/satext_quan.png" alt="unit_teaser_img"
                            style="width: 100%; max-width: none;">
                    </div>
                </div>

                <h3>Real-Text</h3>
                <div class="image-container">
                    <div class="image-content">
                        <img src="images/realtext_quan.png" alt="realtext"
                            style="width: 100%; max-width: none;">
                    </div>
                </div>

            </section>

            <div class="citation add-top-padding">
                <h1 id="citation">Citation</h1>
                <p> If you use this work or find it helpful, please consider citing: </p>
                <pre id="codecell0">
    <!-- @article{kim2025unit,
      title={UniT: Unified Diffusion Transformer for High-Fidelity Text-Aware Image Restoration},
      author={Kim, Jin Hyeon and Cho, Paand Kim, Mungyeom and Park, Junghyun and Park, Seohyun and Kim, Jaeyeong and Yi, Jung and Cho, Seokju and Kim, Seungryong},
      journal={arXiv preprint arXiv:2512.02006},
      year={2025}
    } -->
                </pre>
            </div>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p class="credit">Credit: The design of this project page is inspired by previous academic project pages, such as <a href="https://llm-grounded-diffusion.github.io/" target="_blank">LLM-grounded Diffusion</a>, <a href="https://describe-anything.github.io/" target="_blank">Describe-anything</a>, <a href="https://cvlab-kaist.github.io/VIRAL" target="_blank">VIRAL</a> and <a href="https://cvlab-kaist.github.io/MV-TAP" target="_blank">MV-TAP</a>.</p>
        </div>
    </footer>

    
    <script>
    function changeSlide(button, direction) {
        // Find the slideshow container that this button belongs to
        const container = button.closest(".slideshow-container");
        const slideshow = container.querySelector(".slideshow");
        const slides = slideshow.querySelectorAll(".slide");

        // Find current slide index
        let currentIndex = Array.from(slides).findIndex(slide =>
            slide.classList.contains("active")
        );

        // Remove active from current
        slides[currentIndex].classList.remove("active");

        // Compute next index
        currentIndex += direction;

        if (currentIndex < 0) currentIndex = slides.length - 1;
        if (currentIndex >= slides.length) currentIndex = 0;

        // Add active to new slide
        slides[currentIndex].classList.add("active");
    }
    </script>


</body>
</html>